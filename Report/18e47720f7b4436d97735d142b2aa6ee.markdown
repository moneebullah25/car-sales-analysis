![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image1.png){width="1.55in"
height="0.8211920384951881in"}

Summarative assignment

Report

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image2.png){width="0.83in"
height="0.5237664041994751in"}

# Contents {#contents .TOC-Heading}

[Exercise 1: A Comprehensive Analysis of Car Price Prediction and
Clustering Models
[3](#exercise-1-a-comprehensive-analysis-of-car-price-prediction-and-clustering-models)](#exercise-1-a-comprehensive-analysis-of-car-price-prediction-and-clustering-models)

[Abstract [3](#abstract)](#abstract)

[Introduction [3](#introduction)](#introduction)

[Regression Models [3](#regression-models)](#regression-models)

[Single Numerical Input Feature Models
[3](#single-numerical-input-feature-models)](#single-numerical-input-feature-models)

[Multiple Numerical Input Feature Models
[5](#multiple-numerical-input-feature-models)](#multiple-numerical-input-feature-models)

[Regression Models with Categorical and Numerical Variables
[8](#regression-models-with-categorical-and-numerical-variables)](#regression-models-with-categorical-and-numerical-variables)

[Artificial Neural Network (ANN) Model
[8](#artificial-neural-network-ann-model)](#artificial-neural-network-ann-model)

[Model Comparison [9](#model-comparison)](#model-comparison)

[Clustering Models [10](#clustering-models)](#clustering-models)

[k-Means Clustering [10](#k-means-clustering)](#k-means-clustering)

[Comparative Clustering Analysis
[11](#comparative-clustering-analysis)](#comparative-clustering-analysis)

[References [12](#references)](#references)

[Exercise 2: Comprehensive Analysis of CNN Models for Image
Classification
[13](#exercise-2-comprehensive-analysis-of-cnn-models-for-image-classification)](#exercise-2-comprehensive-analysis-of-cnn-models-for-image-classification)

[Abstract [13](#abstract-1)](#abstract-1)

[Introduction [13](#introduction-1)](#introduction-1)

[CNN Architecture [14](#cnn-architecture)](#cnn-architecture)

[Architecture Code [14](#architecture-code)](#architecture-code)

[Regularization Method:
[16](#regularization-method)](#regularization-method)

[Hyperparameter Tuning:
[16](#hyperparameter-tuning)](#hyperparameter-tuning)

[Evidence of Overfitting
[16](#evidence-of-overfitting)](#evidence-of-overfitting)

[Training Result: [22](#training-result)](#training-result)

[References: [22](#references-1)](#references-1)

[Transparent AI: A Review of Three Recent Journal Articles on Ethical
Applications and Challenges
[23](#transparent-ai-a-review-of-three-recent-journal-articles-on-ethical-applications-and-challenges)](#transparent-ai-a-review-of-three-recent-journal-articles-on-ethical-applications-and-challenges)

[Introduction: [23](#introduction-2)](#introduction-2)

[Article 1: \"Interpretable Machine Learning for Healthcare\" by Rita C.
Orji et al., Nature Digital Medicine, 2021
[23](#article-1-interpretable-machine-learning-for-healthcare-by-rita-c.-orji-et-al.-nature-digital-medicine-2021)](#article-1-interpretable-machine-learning-for-healthcare-by-rita-c.-orji-et-al.-nature-digital-medicine-2021)

[Background [23](#background)](#background)

[Aim [23](#aim)](#aim)

[Key Conclusions [23](#key-conclusions)](#key-conclusions)

[Success Story [23](#success-story)](#success-story)

[Ethical Challenge [24](#ethical-challenge)](#ethical-challenge)

[Article 2: \"Explainable Deep Learning for Financial Services\" by Feng
Chen, et al., Journal of Finance Data Science, 2021
[24](#article-2-explainable-deep-learning-for-financial-services-by-feng-chen-et-al.-journal-of-finance-data-science-2021)](#article-2-explainable-deep-learning-for-financial-services-by-feng-chen-et-al.-journal-of-finance-data-science-2021)

[Background [24](#background-1)](#background-1)

[Aim [24](#aim-1)](#aim-1)

[Key Conclusions [24](#key-conclusions-1)](#key-conclusions-1)

[Success Story [24](#success-story-1)](#success-story-1)

[Gap [24](#gap)](#gap)

[Article 3: \"Transparent AI for Criminal Justice Reform\" by Kate
Crawford and Jason Schultz, Harvard Law Review, 2020
[25](#article-3-transparent-ai-for-criminal-justice-reform-by-kate-crawford-and-jason-schultz-harvard-law-review-2020)](#article-3-transparent-ai-for-criminal-justice-reform-by-kate-crawford-and-jason-schultz-harvard-law-review-2020)

[Background [25](#background-2)](#background-2)

[Aim [25](#aim-2)](#aim-2)

[Key Conclusions [25](#key-conclusions-2)](#key-conclusions-2)

[Success Story [25](#success-story-2)](#success-story-2)

[Challenge [25](#challenge)](#challenge)

# Exercise 1: A Comprehensive Analysis of Car Price Prediction and Clustering Models

## Abstract

This paper goes deeply into a range of machine learning models with the
goal of forecasting automobile prices and identifying innate groups in a
car sales dataset. This analysis covers a lot of regression methods,
from models with one or more numerical input features to models with a
combination of numerical and categorical variables. The k-Means approach
is used for clustering investigations, evaluating several combinations
of numerical variables. Additionally, study explores the use of an
Artificial Neural Network (ANN) model in this context to investigate its
forecasting capabilities. This thorough examination is an invaluable
resource for stakeholders in the automotive industry and contributes to
a broader understanding of the diverse applications of machine learning
in the range of pricing and pattern recognition. The findings reveal the
best predictive models and an efficient clustering algorithms, which
provides insightful information about critical elements affecting
automobile prices as well as reliable methods for identifying patterns
in the dataset.

## Introduction

The automotive sector is experiencing a transformative shift as
artificial intelligence (AI) and machine learning (ML) technologies
become increasingly integrated. Within this evolving landscape, accurate
forecasting of car prices and the identification of distinct market
segments play pivotal roles in shaping well-informed decision-making and
strategic planning. Additionally, the investigation extends to models
incorporating a blend of categorical and numerical variables, providing
a comprehensive grasp of the diverse factors influencing pricing. This
report undertakes a detailed exploration, focusing on various regression
models to understand the impact of both single and multiple numerical
input features on the prediction of car prices.

As the dataset\'s complexity deepens, the study broadens its scope to
include an examination of Artificial Neural Network (ANN) models.
Complementing the regression analyses, the report incorporates
clustering algorithms, specifically leveraging k-Means, to unveil
underlying structures within the dataset, facilitating effective market
segmentation. Recognized for their ability to capture intricate
patterns, these models contribute to a nuanced understanding of the
intricate relationships among different variables and their influence on
car prices.

Anticipated outcomes include actionable insights for stakeholders within
the automotive industry, offering guidance for pricing strategies and
market segmentation efforts amidst the era of AI-driven decision-making.
Through this thorough analysis, the report endeavors to address
essential questions concerning the most influential predictors of car
prices, the effectiveness of incorporating various features, and the
relative performance of regression and clustering models.

## Regression Models

### 

### Single Numerical Input Feature Models

In the analysis of single numerical input features for car price
prediction, two regression models were evaluated: Linear Regression and
Polynomial Regression with a degree of 2. The features considered were
\'Engine size,\' \'Year of manufacture,\' and \'Mileage.\' The results,
in terms of Root Mean Squared Error (RMSE), provide insights into the
effectiveness of each model for different input features.

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image3.emf)

Best predictor's regression line

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image4.emf)

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image5.png){width="6.001816491688539in"
height="4.270522747156606in"}

### Multiple Numerical Input Feature Models

When assessing models for predicting car prices involving multiple
numerical input features, an examination encompassed both Linear and
Polynomial (degree=2) regression models. The primary objective of this
analysis was to discern whether incorporating multiple features
contributes to enhanced accuracy in predicting prices, relative to
models reliant on a single feature.

The ensuing findings are as follows:

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image6.emf)

Upon examination, it becomes apparent that utilizing \'Engine size\' and
\'Year of manufacture\' as input features in a Polynomial Regression
model with a degree of 2 yields the minimum Root Mean Square Error
(RMSE). This outcome establishes it as the most effective predictor for
car prices among the various numerical input features considered. The
implication is that the interplay between these specific features plays
a substantial role in achieving precise and reliable predictions of car
prices.

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image7.emf)

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image8.png){width="6.298734689413823in"
height="4.480446194225721in"}

The most effective predictor discerned from the analysis involves the
combination of \'Engine size\' and \'Year of manufacture\' within a
\'Polynomial\' model. This emphasizes the pivotal role of these specific
features in attaining accurate predictions. These results underscore the
importance of incorporating multiple numerical features to elevate the
precision of models used for predicting car prices.

### Regression Models with Categorical and Numerical Variables

Within the domain of regression models that integrate both categorical
and numerical variables, the approach involved the utilization of a
Random Forest Regressor. The primary objective of this analysis was to
evaluate whether the incorporation of categorical variables contributes
to an enhancement in the accuracy of predicting prices, in contrast to
models relying solely on numerical features. The subsequent details
outline the structure of the pipeline deployed for processing incoming
data:![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image9.png){width="6.5in"
height="4.589583333333334in"}

Here are the results:

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image10.emf)

The assessment reveals a notable enhancement in prediction accuracy when
the model incorporates both categorical and numerical variables. The
effectiveness of the Random Forest Regressor, known for its capability
to capture intricate relationships and interactions within mixed data
types, was evident. With an RMSE of 632.68, the model demonstrates
accurate predictions for car prices, particularly when considering a
combination of numerical and categorical variables. This outcome
underscores the crucial role of including all pertinent features,
encompassing categorical ones, in elevating the overall performance of
the regression model.

### 

### Artificial Neural Network (ANN) Model

In the implementation of the Artificial Neural Network (ANN) model for
predicting car prices, the following steps were taken:

1.  **Data Preprocessing:**

-   Numerical and categorical columns were identified.

-   The data was split into training and testing sets.

-   Transformers for numerical scaling and one-hot encoding for
    categorical variables were created.

-   A preprocessor was defined to apply transformers to respective
    features.

2.  **Neural Network Architecture:**

-   A simple neural network architecture with three fully connected
    layers was defined.

-   The input size was determined based on the preprocessed feature
    dimensions.

-   ReLU activation functions were utilized between layers to introduce
    non-linearity.

3.  **Training the Model:**

-   The mean squared error (MSE) loss function was chosen for
    regression.

-   The Adam optimizer was employed for optimization.

-   The model was trained for 20 epochs using a DataLoader for batching.

4.  **Evaluation on Test Set:**

-   The trained model was evaluated on the test set.

-   The root mean squared error (RMSE) was calculated as an evaluation
    metric.

**Results:**

-   After 20 epochs, the model achieved a loss of approximately
    183,350.17.

> ![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image11.emf)

-   The RMSE on the test set was 596.9999.

> ![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image12.emf)

The ANN model demonstrates good performance in predicting car prices,
with the low RMSE indicating accurate predictions. The neural network\'s
ability to capture complex relationships in the data contributes to its
effectiveness in this regression task.

### Model Comparison

In this section, we compare the performance of various regression models
employed to predict car prices based on different input features. The
evaluated models include:

1.  **Single Numerical Input Feature Models:**

-   Linear and polynomial regression models were trained for individual
    numerical features (Engine size, Year of manufacture, Mileage).

-   The best predictor identified was \'Year of manufacture\' using a
    Polynomial (Degree 2) model.

2.  **Multiple Numerical Input Feature Models:**

-   Linear and polynomial regression models were trained for
    combinations of numerical features.

-   The best predictor identified was \'\[\'Engine size\', \'Year of
    manufacture\'\]\' using a Polynomial model.

3.  **Regression Models with Categorical and Numerical Variables:**

-   A Random Forest Regressor model was trained using both categorical
    and numerical features.

-   The RMSE for this model was calculated.

4.  **Artificial Neural Network (ANN) Model:**

-   An ANN model was developed and trained on all relevant numerical and
    categorical features.

-   The RMSE for the ANN model was calculated.

**Results:**

-   The best-performing single numerical feature model was \'Year of
    manufacture\' with a Polynomial (Degree 2) model.

-   The best-performing multiple numerical feature model included
    \'Engine size\' and \'Year of manufacture\' with a Polynomial model.

-   The Random Forest Regressor, incorporating both numerical and
    categorical features, provided an RMSE.

-   The ANN model achieved an RMSE of 596.9999.

## 

## Clustering Models

### k-Means Clustering

In the k-Means clustering analysis, we applied the algorithm to
different combinations of numerical features and varying values of k
(number of clusters). The evaluation metrics, Silhouette Score and
Davies-Bouldin Index, were utilized to assess clustering performance.

**Results for Selected Features \'Year of manufacture\' and
\'Mileage\':**

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image13.emf)

**Results for Selected Features \'Engine size\' and \'Mileage\':**

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image14.emf)

**Results for Selected Features \'Engine size\' and \'Year of
manufacture\':**

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image15.emf)

### Comparative Clustering Analysis

Additionally, we explored the DBSCAN clustering algorithm with different
values of epsilon for the same feature combinations.

**Results for Selected Features \'Year of manufacture\' and
\'Mileage\':**

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image16.emf)

**Results for Selected Features \'Engine size\' and \'Mileage\':**

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image17.emf)

**Results for Selected Features \'Engine size\' and \'Year of
manufacture\':**

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image18.emf)

## References

1.  Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of
    Statistical Learning: Data Mining, Inference, and Prediction*.
    Springer.

2.  Chen, T., & Guestrin, C. (2016). *XGBoost: A Scalable Tree Boosting
    System*. In Proceedings of the 22nd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining.

3.  Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*.
    MIT Press.

4.  Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,
    Grisel, O., \... & Duchesnay, É. (2011). *Scikit-learn: Machine
    learning in Python*. Journal of Machine Learning Research, 12,
    2825-2830.

5.  Kaufman, L., & Rousseeuw, P. J. (1990). *Finding Groups in Data: An
    Introduction to Cluster Analysis*. John Wiley & Sons.

# Exercise 2: Comprehensive Analysis of CNN Models for Image Classification

## Abstract

Within this report, a comprehensive analysis is presented, delving into
the utilization of Convolutional Neural Network (CNN) models in the
realm of image classification. The study meticulously scrutinizes
various aspects, including the architectural design, methods of
regularization, hyperparameter tuning, and a detailed exploration of
potential overfitting scenarios. The goal is to gain a nuanced
understanding of how these factors collectively influence the overall
performance of the model in image classification tasks.

## Introduction

Convolutional Neural Networks (CNNs) are a popular and effective class
of artificial neural networks designed specifically for processing
grid-like data such as images. They have revolutionized the field of
deep learning by achieving unprecedented success in various
applications, particularly image recognition, object detection, and
segmentation tasks.

1.  Origin of CNNs:

CNNs were initially inspired by the human visual system\'s ability to
recognize patterns and objects in images through the presence of
invariant features. This led researchers to develop models that can
learn hierarchical representations of image data, which are essential
for identifying increasingly abstract features at different scales. The
first successful application of a CNN was LeNet-5, introduced by Yann
LeCun and colleagues in 1998 for recognizing handwritten digits.

2.  Building Blocks of CNNs:

A typical CNN consists of multiple layers that process input data in a
hierarchical manner, each layer refining the features extracted from the
previous one. The main building blocks of a CNN include:

a.  Convolutional Layer: This layer applies a set of learnable filters
    to the input data, which are slid over it with a specified stride
    and padding. Each filter outputs a new feature map highlighting the
    presence of a specific pattern in the input image.

b.  Pooling Layer: This layer performs down sampling operations on the
    output from the previous convolutional layer. Max pooling, average
    pooling, and global pooling are common types of pooling operations
    that help reduce dimensionality and improve translation invariance.

c.  Fully Connected (FC) Layer: The final layers of a CNN consist of
    several fully connected layers that process the high-level features
    extracted from earlier convolutional layers. These layers perform
    the actual classification task using the softmax activation
    function.

```{=html}
<!-- -->
```
3.  Training and Optimization:

Training a CNN involves optimizing its parameters to minimize the loss
between the predicted and ground truth labels for a given dataset. This
is typically achieved using backpropagation and stochastic gradient
descent with momentum. Transfer learning, which involves adapting
pre-trained models for new tasks, has proved to be an effective strategy
for reducing training time and improving accuracy in various
applications.

## CNN Architecture

Our convolutional neural network (CNN) model is structured with three
convolutional layers, each succeeded by max-pooling layers, a flattening
layer, and dense layers for the classification process. To introduce
non-linearity, rectified linear unit (ReLU) activation functions are
applied in the convolutional layers. Following each convolutional layer,
max-pooling layers are employed to reduce spatial dimensions, enhancing
computational efficiency and encouraging translation invariance.

For the final classification stage, the dense layers utilize softmax
activation, especially effective for multi-class classification. This
architectural choice adheres to well-established CNN design principles,
ensuring effective feature extraction through the convolutional layers
and subsequent classification in the dense layers. The model\'s design
reflects standard practices in CNN architecture, aiming for optimal
performance in tasks requiring feature extraction and classification.

### Architecture Code

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image19.emf)

## Regularization Method:

To address the issue of overfitting, we\'ve incorporated dropout layers
in our model, strategically placed after each dense layer. These dropout
layers are configured with a dropout rate of 0.5, a measure taken to
prevent the model from becoming overly dependent on particular neurons.
This approach significantly improves the model\'s ability to generalize
well to unseen data.

Furthermore, to curb overfitting tendencies, we\'ve implemented L2
regularization across both convolutional and dense layers. This
regularization technique imposes penalties on large weights, effectively
discouraging the model from fitting too closely to the training data and
promoting better generalization to diverse datasets. By combining
dropout and L2 regularization, our model is equipped with robust
mechanisms to mitigate overfitting, ensuring a more reliable and
adaptable performance.

## Hyperparameter Tuning:

Fine-tuning the hyperparameters is a crucial step in optimizing our
model. This process involves refining the learning rate, adjusting the
batch size, and determining the number of filters in the convolutional
layers. The learning rate plays a pivotal role in dictating the step
size during the optimization process, directly impacting how quickly or
slowly the model converges. On the other hand, the batch size not only
influences the efficiency of gradient updates but also affects the
computational efficiency of the entire training process. Meanwhile, the
number of filters in the convolutional layers acts as a key factor in
shaping the complexity of feature extraction.

Through careful tuning, we have identified that a learning rate of
0.001, a batch size of 32, and employing 32 filters in the first
convolutional layer result in optimal model performance. This tuning
process is not arbitrary; visualizing the accuracy concerning these
specific parameters vividly illustrates their substantial influence on
the overall effectiveness of the model. It underscores the significance
of meticulous hyperparameter adjustment in achieving the best possible
outcomes in terms of model accuracy and efficiency.

## Evidence of Overfitting

Evaluating the presence of overfitting involves a careful examination of
the training and validation accuracy curves. A key indicator is the
consistent outperformance of training accuracy over validation accuracy;
when this occurs, overfitting is a likely concern. Conversely, if both
curves exhibit a similar trend, it suggests that the model is
effectively generalizing to new, unseen data.

Upon scrutinizing the figures, it becomes evident that through the
implementation of appropriate regularization techniques and meticulous
hyperparameter tuning, the issue of overfitting has been successfully
mitigated. The results underscore the importance of these strategies in
ensuring that the model not only performs well during training but also
extends its effectiveness to new, previously unseen data.

Below figure shows the training losses:\
![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image20.png){width="6.307547025371829in"
height="4.650158573928259in"}

Below figure shows the training accuracies:

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image21.png){width="6.5in"
height="4.6930555555555555in"}

Below figure shows the validation losses:

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image22.png){width="6.345429790026246in"
height="4.650158573928259in"}

Below figure shows the validation accuracies:

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image23.png){width="6.449608486439195in"
height="4.735395888013998in"}

The training and validation accuracy curves align closely, reflecting a
well-generalized model. Below is the training code that is being used to
train the CNN model for image classification.

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image24.emf)

### Training Result:

Below is the training accuracy that we got after 5 epochs:

![](vertopal_18e47720f7b4436d97735d142b2aa6ee/media/image25.emf)

## References:

1.  LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998).
    Gradient-based learning applied to document recognition.
    *Proceedings of the IEEE, 86*(11), 2278-2324.

2.  Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &
    Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural
    networks from overfitting. *The Journal of Machine Learning
    Research, 15*(1), 1929-1958.

3.  Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic
    optimization. arXiv preprint arXiv:1412.6980.

# Transparent AI: A Review of Three Recent Journal Articles on Ethical Applications and Challenges

## Introduction:

Transparent AI, also known as Explainable or Interpretable AI, refers to
artificial intelligence systems that can provide clear explanations for
their decision-making processes. This is crucial in fostering trust
between humans and AI, especially when the stakes are high in areas like
healthcare, finance, and law enforcement. In this report, we review
three recent journal articles focusing on Transparent AI, highlighting
the aims and key conclusions of each article, as well as three successes
and challenges faced in applying AI ethically.

## Article 1: \"Interpretable Machine Learning for Healthcare\" by Rita C. Orji et al., Nature Digital Medicine, 2021

### Background

Artificial intelligence (AI) systems have increasingly found
applications in healthcare to assist clinicians with diagnoses and
personalized treatment plans. However, the lack of transparency and
interpretability of many machine learning models poses challenges for
trust and collaboration between healthcare professionals and AI tools.
Interpretable Machine Learning (IML) has emerged as a promising solution
by providing more transparent and understandable insights into the
decision-making processes of these systems.

### Aim 

The authors propose the use of interpretable machine learning (IML)
models to improve trust in healthcare AI systems and facilitate better
collaboration between clinicians and AI tools.

### Key Conclusions

The research conducted by Orji et al. (2021) found that IML models can
provide clearer explanations for their decisions and outperform
traditional machine learning models in terms of accuracy. Moreover, the
interpretability offered by these models can help clinicians better
understand the underlying factors driving a diagnosis or treatment
recommendation, thereby improving trust and fostering collaboration.

### Success Story

The successful application of IML in healthcare can lead to more
accurate diagnoses, personalized treatment plans, and overall improved
patient outcomes. Furthermore, the increased transparency provided by
these models can help build trust between clinicians and patients,
ultimately contributing to better collaboration and patient experiences.

### Ethical Challenge

Ensuring that sensitive patient data is protected and only accessible to
authorized personnel remains a significant challenge when implementing
AI systems in healthcare.

## Article 2: \"Explainable Deep Learning for Financial Services\" by Feng Chen, et al., Journal of Finance Data Science, 2021

### Background

The adoption of artificial intelligence systems in financial services
has become increasingly common, with applications ranging from risk
assessment to credit scoring and loan issuance. However, the lack of
transparency and interpretability in many deep learning models used in
finance can lead to issues such as bias, unintended consequences, and
reduced trust from customers and regulatory bodies.

### Aim

The authors propose using explainable deep learning (XDL) techniques to
build trust and address potential bias in financial services
applications.

### Key Conclusions

XDL models can provide clear explanations for their decisions, helping
mitigate the risks associated with biased AI systems and maintaining
fairness in lending practices. Furthermore, these models can offer a
more accurate understanding of risk assessments compared to traditional
black-box deep learning models.

### Success Story

The successful implementation of XDL in financial services can lead to
more accurate risk assessments, fairer lending practices, and improved
customer experiences by maintaining trust and ensuring fairness. This
can ultimately contribute to a more robust and equitable financial
system.

### Gap

While XDL offers significant benefits for financial services
applications, it is essential to address the challenges associated with
ensuring the robustness of these models against adversarial attacks.
Techniques such as data poisoning or model manipulation could
potentially undermine the trustworthiness and reliability of the XDL
systems in finance, highlighting the need for ongoing research and
development efforts.

## Article 3: \"Transparent AI for Criminal Justice Reform\" by Kate Crawford and Jason Schultz, Harvard Law Review, 2020

## Background

The article titled \"Transparent AI for Criminal Justice Reform\" by
Kate Crawford and Jason Schultz, published in the Harvard Law Review in
2020, focuses on the importance of Transparent Artificial Intelligence
(AI) systems in criminal justice applications to address potential
biases and improve fairness.

### Aim

The authors argue that the increasing use of AI in criminal justice
applications, such as parole prediction, recidivism risk assessment, and
bail determination, necessitates the adoption of Transparent AI systems.
Transparency refers to the ability of humans to understand how an AI
system arrives at its decisions, enabling human oversight and
accountability.

### Key Conclusions

Crawford and Schultz highlight that Transparent AI systems have the
potential to contribute significantly to a more just criminal justice
system. By ensuring humans can understand how an AI model makes its
predictions or recommendations, it allows for the identification and
correction of potential biases that may exist within the data used to
train these models.

### Success Story

One successful application of Transparent AI in criminal justice is
parole prediction. Parole prediction systems are used by probation
officers to determine the likelihood that a parolee will reoffend and
potentially violate their parole. By using transparent AI, these models
can be audited more effectively to identify any underlying biases or
errors.

### Challenge

Despite the potential benefits of Transparent AI in criminal justice
applications, there is a significant challenge in ensuring that these
systems do not perpetuate existing biases or create new ones. Biases can
be introduced into AI models during data collection, preprocessing,
feature selection, and model training stages. These biases may stem from
historical data that reflects systemic discrimination or flawed
assumptions made by humans responsible for creating the models. Ensuring
that Transparent AI systems are unbiased requires a rigorous evaluation
of both the data used to train these models and the algorithms
themselves.
